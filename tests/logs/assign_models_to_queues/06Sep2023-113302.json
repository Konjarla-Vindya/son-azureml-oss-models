{
    "test-southcentralus": {
        "0": [
            "SamLowe/roberta-base-go_emotions",
            "benjamin/wtp-canine-s-1l",
            "PascalNotin/Tranception_Small",
            "YeungNLP/firefly-baichuan-13b",
            "cardiffnlp/twitter-xlm-roberta-base-sentiment",
            "cardiffnlp/twitter-roberta-base-offensive",
            "microsoft/mdeberta-v3-base",
            "vblagoje/bert-english-uncased-finetuned-pos",
            "syzymon/long_llama_3b",
            "bigscience/bloomz-1b1",
            "cross-encoder/ms-marco-MiniLM-L-6-v2",
            "ipuneetrathore/bert-base-cased-finetuned-finBERT",
            "tomh/toxigen_roberta",
            "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
            "finiteautomata/bertweet-base-sentiment-analysis",
            "deepset/bert-large-uncased-whole-word-masking-squad2",
            "NousResearch/Llama-2-7b-hf"
        ],
        "1": [
            "cardiffnlp/twitter-roberta-base-irony",
            "microsoft/deberta-base",
            "bigscience/bloom-560m",
            "cardiffnlp/twitter-roberta-base-sentiment-latest",
            "dslim/bert-base-NER",
            "sshleifer/distilbart-cnn-12-6",
            "dslim/bert-large-NER",
            "hfl/chinese-bert-wwm-ext",
            "dmis-lab/biobert-large-cased-v1.1-mnli",
            "hf-internal-testing/tiny-xlm-roberta",
            "michiyasunaga/BioLinkBERT-base",
            "TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ",
            "Qwen/Qwen-7B",
            "vinai/bertweet-base",
            "michellejieli/emotion_text_classifier",
            "meta-llama/Llama-2-70b-chat-hf",
            "hf-internal-testing/tiny-random-OPTForCausalLM"
        ],
        "2": [
            "marieke93/MiniLM-evidence-types",
            "QCRI/bert-base-multilingual-cased-pos-english",
            "baichuan-inc/Baichuan-13B-Chat",
            "emilyalsentzer/Bio_ClinicalBERT",
            "dslim/bert-base-NER-uncased",
            "facebook/opt-6.7b",
            "lmsys/vicuna-7b-v1.1",
            "cl-tohoku/bert-base-japanese",
            "meta-llama/Llama-2-7b-hf",
            "facebook/esm2_t6_8M_UR50D",
            "tiiuae/falcon-40b-instruct",
            "facebook/opt-125m",
            "sshleifer/tiny-gpt2",
            "microsoft/deberta-v3-base",
            "mosaicml/mpt-7b",
            "meta-llama/Llama-2-13b-chat-hf",
            "microsoft/infoxlm-large"
        ],
        "3": [
            "alimazhar-110/website_classification",
            "NousResearch/Llama-2-13b-hf",
            "ckiplab/bert-base-chinese-ner",
            "yiyanghkust/finbert-tone",
            "bhadresh-savani/distilbert-base-uncased-emotion",
            "dbmdz/bert-large-cased-finetuned-conll03-english",
            "distilbert-base-cased",
            "facebook/opt-1.3b",
            "meta-llama/Llama-2-7b-chat-hf",
            "EleutherAI/pythia-6.9b",
            "cl-tohoku/bert-base-japanese-whole-word-masking",
            "klue/roberta-base",
            "fxmarty/tiny-llama-fast-tokenizer",
            "tiiuae/falcon-7b-instruct",
            "decapoda-research/llama-7b-hf",
            "seyonec/ChemBERTa-zinc-base-v1",
            "kk08/CryptoBERT"
        ],
        "4": [
            "Ashishkr/query_wellformedness_score",
            "tiiuae/falcon-7b",
            "deepset/roberta-base-squad2",
            "nlptown/bert-base-multilingual-uncased-sentiment",
            "j-hartmann/emotion-english-distilroberta-base",
            "lvwerra/distilbert-imdb",
            "cross-encoder/ms-marco-MiniLM-L-12-v2",
            "microsoft/deberta-large-mnli",
            "hf-internal-testing/tiny-random-GPTNeoXForCausalLM",
            "huggyllama/llama-7b",
            "cross-encoder/ms-marco-MiniLM-L-4-v2",
            "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
            "Seethal/sentiment_analysis_generic_dataset",
            "dmis-lab/biobert-base-cased-v1.2",
            "Qwen/Qwen-7B-Chat",
            "amazon/FalconLite"
        ],
        "5": [
            "vinai/xphonebert-base",
            "mosaicml/mpt-7b-instruct",
            "baichuan-inc/Baichuan-13B-Base",
            "facebook/bart-large-cnn",
            "martin-ha/toxic-comment-model",
            "cardiffnlp/twitter-roberta-base-sentiment",
            "cerebras/btlm-3b-8k-base",
            "stabilityai/StableBeluga2",
            "cross-encoder/ms-marco-TinyBERT-L-2-v2",
            "ai4bharat/IndicNER",
            "databricks/dolly-v2-3b",
            "philschmid/bart-large-cnn-samsum",
            "madhurjindal/autonlp-Gibberish-Detector-492513457",
            "TheBloke/Llama-2-70B-GPTQ",
            "pythainlp/thainer-corpus-v2-base-model",
            "skt/kogpt2-base-v2"
        ]
    }
}